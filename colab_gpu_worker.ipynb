{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ PyReflect GPU Worker\n",
    "\n",
    "This notebook connects to your VPS to process training jobs on Colab's free GPU.\n",
    "\n",
    "**How it works:**\n",
    "1. Connects to your Redis queue on the VPS\n",
    "2. Polls for queued training jobs\n",
    "3. Runs training on Colab GPU\n",
    "4. Sends results back through Redis\n",
    "\n",
    "**Setup:**\n",
    "1. Update the `REDIS_URL` below with your VPS credentials\n",
    "2. Enable GPU runtime: `Runtime > Change runtime type > T4 GPU`\n",
    "3. Run all cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Update these settings to match your VPS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURATION =====\n",
    "# Update this to your VPS Redis URL\n",
    "# Format: redis://:PASSWORD@YOUR_VPS_IP:6379\n",
    "REDIS_URL = \"redis://:your_redis_password@YOUR_VPS_IP:6379\"\n",
    "\n",
    "# Queue name (should match your backend)\n",
    "QUEUE_NAME = \"training\"\n",
    "\n",
    "# Worker settings\n",
    "POLL_INTERVAL = 5  # seconds between polls when idle\n",
    "WORKER_NAME = \"colab-gpu-worker\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install redis rq torch numpy pymongo huggingface_hub pyreflect-ml -q\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"‚úÖ GPU available: {gpu_name}\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Go to Runtime > Change runtime type > T4 GPU\")\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connect to Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis import Redis\n",
    "from rq import Queue, Worker\n",
    "\n",
    "try:\n",
    "    redis_conn = Redis.from_url(REDIS_URL)\n",
    "    redis_conn.ping()\n",
    "    print(f\"‚úÖ Connected to Redis!\")\n",
    "    \n",
    "    queue = Queue(QUEUE_NAME, connection=redis_conn)\n",
    "    print(f\"üìã Queue '{QUEUE_NAME}' has {len(queue)} jobs waiting\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to connect: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check your REDIS_URL is correct\")\n",
    "    print(\"2. Ensure Redis is running on your VPS\")\n",
    "    print(\"3. Verify firewall allows port 6379\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Job Code\n",
    "\n",
    "This is a copy of your backend's `run_training_job` function, adapted for Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "from datetime import datetime, timezone\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "\n",
    "# Import pyreflect components\n",
    "from pyreflect import ReflectivityDataGenerator, DataProcessor, CNN\n",
    "try:\n",
    "    from pyreflect import compute_nr_from_sld\n",
    "    COMPUTE_NR_AVAILABLE = True\n",
    "except ImportError:\n",
    "    COMPUTE_NR_AVAILABLE = False\n",
    "\n",
    "# Training constants (matching your backend defaults)\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "\n",
    "def _compute_norm_stats(curves: np.ndarray) -> dict:\n",
    "    \"\"\"Compute normalization statistics for curves.\"\"\"\n",
    "    x_points = curves[:, 0, :]\n",
    "    y_points = curves[:, 1, :]\n",
    "    return {\n",
    "        \"x\": {\"min\": float(np.min(x_points)), \"max\": float(np.max(x_points))},\n",
    "        \"y\": {\"min\": float(np.min(y_points)), \"max\": float(np.max(y_points))},\n",
    "    }\n",
    "\n",
    "\n",
    "def run_training_job(\n",
    "    job_params: dict[str, Any],\n",
    "    *,\n",
    "    user_id: str | None = None,\n",
    "    name: str | None = None,\n",
    "    hf_config: dict | None = None,\n",
    "    mongo_uri: str | None = None,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run a training job on Colab GPU.\n",
    "    \n",
    "    This is adapted from your backend's run_training_job function.\n",
    "    \"\"\"\n",
    "    from rq import get_current_job\n",
    "    \n",
    "    job = get_current_job()\n",
    "    logs = []\n",
    "    \n",
    "    def log(message: str):\n",
    "        \"\"\"Log message to console and job meta.\"\"\"\n",
    "        print(message)\n",
    "        logs.append(message)\n",
    "        if job:\n",
    "            job.meta[\"logs\"] = logs\n",
    "            job.meta[\"updated_at\"] = datetime.now(timezone.utc).isoformat()\n",
    "            job.save_meta()\n",
    "    \n",
    "    def update_progress(epoch: int, total: int, train_loss: float, val_loss: float):\n",
    "        if job:\n",
    "            job.meta[\"progress\"] = {\n",
    "                \"epoch\": epoch,\n",
    "                \"total\": total,\n",
    "                \"trainLoss\": train_loss,\n",
    "                \"valLoss\": val_loss,\n",
    "            }\n",
    "            job.save_meta()\n",
    "    \n",
    "    # Initialize job meta\n",
    "    if job:\n",
    "        job.meta[\"status\"] = \"initializing\"\n",
    "        job.meta[\"logs\"] = logs\n",
    "        if user_id:\n",
    "            job.meta[\"user_id\"] = user_id\n",
    "        if name:\n",
    "            job.meta[\"name\"] = name\n",
    "        job.meta[\"started_at\"] = datetime.now(timezone.utc).isoformat()\n",
    "        job.save_meta()\n",
    "    \n",
    "    # Extract parameters\n",
    "    gen_params = job_params.get(\"generator\", {})\n",
    "    train_params = job_params.get(\"training\", {})\n",
    "    \n",
    "    num_curves = gen_params.get(\"numCurves\", 1000)\n",
    "    num_film_layers = gen_params.get(\"numFilmLayers\", 3)\n",
    "    epochs = train_params.get(\"epochs\", 50)\n",
    "    batch_size = train_params.get(\"batchSize\", 32)\n",
    "    layers = train_params.get(\"layers\", [512, 256, 128])\n",
    "    dropout = train_params.get(\"dropout\", 0.2)\n",
    "    \n",
    "    total_start = time.perf_counter()\n",
    "    \n",
    "    # =====================\n",
    "    # Data Generation\n",
    "    # =====================\n",
    "    log(f\"üîÑ Generating {num_curves} synthetic curves with {num_film_layers} film layers...\")\n",
    "    if job:\n",
    "        job.meta[\"status\"] = \"generating\"\n",
    "        job.save_meta()\n",
    "    \n",
    "    gen_start = time.perf_counter()\n",
    "    data_generator = ReflectivityDataGenerator(num_layers=num_film_layers)\n",
    "    nr_curves, sld_curves = data_generator.generate(num_curves)\n",
    "    gen_time = time.perf_counter() - gen_start\n",
    "    \n",
    "    log(f\"   Generated NR shape: {nr_curves.shape}, SLD shape: {sld_curves.shape}\")\n",
    "    log(f\"   Generation took {gen_time:.2f}s\")\n",
    "    \n",
    "    # =====================\n",
    "    # Preprocessing\n",
    "    # =====================\n",
    "    log(\"üìä Preprocessing data...\")\n",
    "    if job:\n",
    "        job.meta[\"status\"] = \"preprocessing\"\n",
    "        job.save_meta()\n",
    "    \n",
    "    nr_log = np.array(nr_curves, copy=True)\n",
    "    nr_log[:, 1, :] = np.log10(np.clip(nr_log[:, 1, :], 1e-8, None))\n",
    "    nr_stats = _compute_norm_stats(nr_log)\n",
    "    normalized_nr = DataProcessor.normalize_xy_curves(nr_curves, apply_log=True, min_max_stats=nr_stats)\n",
    "    \n",
    "    sld_stats = _compute_norm_stats(sld_curves)\n",
    "    normalized_sld = DataProcessor.normalize_xy_curves(sld_curves, apply_log=False, min_max_stats=sld_stats)\n",
    "    \n",
    "    reshaped_nr = normalized_nr[:, 1:2, :]\n",
    "    \n",
    "    # =====================\n",
    "    # Training\n",
    "    # =====================\n",
    "    log(f\"üèãÔ∏è Training CNN model ({epochs} epochs, batch size {batch_size})...\")\n",
    "    if job:\n",
    "        job.meta[\"status\"] = \"training\"\n",
    "        job.save_meta()\n",
    "    \n",
    "    model = CNN(layers=layers, dropout_prob=dropout).to(DEVICE)\n",
    "    model.train()\n",
    "    \n",
    "    list_arrays = DataProcessor.split_arrays(reshaped_nr, normalized_sld, size_split=SPLIT_RATIO)\n",
    "    tensor_arrays = DataProcessor.convert_tensors(list_arrays)\n",
    "    _, _, _, train_loader, valid_loader, _ = DataProcessor.get_dataloaders(*tensor_arrays, batch_size=batch_size)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    epoch_list = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    training_start = time.perf_counter()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in valid_loader:\n",
    "                X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "                outputs = model(X_batch)\n",
    "                val_running_loss += loss_fn(outputs, y_batch).item()\n",
    "        val_loss = val_running_loss / len(valid_loader)\n",
    "        \n",
    "        epoch_list.append(epoch + 1)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        update_progress(epoch + 1, epochs, train_loss, val_loss)\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            log(f\"   Epoch {epoch + 1}/{epochs} - Train: {train_loss:.6f}, Val: {val_loss:.6f}\")\n",
    "    \n",
    "    training_time = time.perf_counter() - training_start\n",
    "    log(f\"   Training completed in {training_time:.2f}s\")\n",
    "    \n",
    "    # =====================\n",
    "    # Inference\n",
    "    # =====================\n",
    "    log(\"üîç Running inference on test sample...\")\n",
    "    if job:\n",
    "        job.meta[\"status\"] = \"inference\"\n",
    "        job.save_meta()\n",
    "    \n",
    "    split_idx = int(len(nr_curves) * SPLIT_RATIO)\n",
    "    test_idx = split_idx\n",
    "    \n",
    "    gt_nr = nr_curves[test_idx]\n",
    "    gt_sld = sld_curves[test_idx]\n",
    "    \n",
    "    inference_start = time.perf_counter()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_nr_normalized = normalized_nr[test_idx : test_idx + 1, 1:2, :]\n",
    "        test_input = torch.tensor(test_nr_normalized, dtype=torch.float32).to(DEVICE)\n",
    "        pred_sld_normalized = model(test_input).cpu().numpy()\n",
    "    \n",
    "    pred_sld_denorm = DataProcessor.denormalize_xy_curves(pred_sld_normalized, stats=sld_stats, apply_exp=False)\n",
    "    pred_sld_y = pred_sld_denorm[0, 1, :]\n",
    "    pred_sld_z = pred_sld_denorm[0, 0, :]\n",
    "    \n",
    "    sld_z = np.linspace(0, 450, len(gt_sld[1]))\n",
    "    \n",
    "    # Compute NR from predicted SLD\n",
    "    computed_nr = gt_nr[1].tolist()\n",
    "    if COMPUTE_NR_AVAILABLE:\n",
    "        log(\"   Computing NR from predicted SLD...\")\n",
    "        try:\n",
    "            pred_sld_profile = (pred_sld_z, pred_sld_y)\n",
    "            _, computed_r = compute_nr_from_sld(pred_sld_profile, Q=gt_nr[0], order=\"substrate_to_air\")\n",
    "            computed_nr = computed_r.tolist()\n",
    "        except Exception as exc:\n",
    "            log(f\"   Warning: Could not compute NR: {exc}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    sample_indices = np.linspace(0, len(pred_sld_y) - 1, 50, dtype=int)\n",
    "    chi = [\n",
    "        {\"x\": int(i), \"predicted\": float(pred_sld_y[idx]), \"actual\": float(gt_sld[1][idx])}\n",
    "        for i, idx in enumerate(sample_indices)\n",
    "    ]\n",
    "    \n",
    "    final_mse = val_losses[-1] if val_losses else 0.0\n",
    "    r2 = 1 - (final_mse / np.var(normalized_sld[:, 1, :]))\n",
    "    mae = float(np.mean(np.abs(pred_sld_y - gt_sld[1])))\n",
    "    inference_time = time.perf_counter() - inference_start\n",
    "    total_time = time.perf_counter() - total_start\n",
    "    \n",
    "    model_id = str(uuid.uuid4())\n",
    "    \n",
    "    log(f\"‚úÖ Complete! Total time: {total_time:.2f}s\")\n",
    "    log(f\"   MSE: {final_mse:.6f}, R¬≤: {r2:.4f}, MAE: {mae:.4f}\")\n",
    "    \n",
    "    # =====================\n",
    "    # Build Result\n",
    "    # =====================\n",
    "    result = {\n",
    "        \"nr\": {\"q\": gt_nr[0].tolist(), \"groundTruth\": gt_nr[1].tolist(), \"computed\": computed_nr},\n",
    "        \"sld\": {\"z\": sld_z.tolist(), \"groundTruth\": gt_sld[1].tolist(), \"predicted\": pred_sld_y.tolist()},\n",
    "        \"training\": {\"epochs\": epoch_list, \"trainingLoss\": train_losses, \"validationLoss\": val_losses},\n",
    "        \"chi\": chi,\n",
    "        \"metrics\": {\"mse\": float(final_mse), \"r2\": float(np.clip(r2, 0, 1)), \"mae\": mae},\n",
    "        \"name\": name,\n",
    "        \"model_id\": model_id,\n",
    "        \"timing\": {\n",
    "            \"generation\": gen_time,\n",
    "            \"training\": training_time,\n",
    "            \"inference\": inference_time,\n",
    "            \"total\": total_time,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    # Save to MongoDB if configured\n",
    "    runtime_user_id = None\n",
    "    runtime_name = name\n",
    "    if job:\n",
    "        runtime_user_id = (job.meta or {}).get(\"user_id\") or user_id\n",
    "        runtime_name = (job.meta or {}).get(\"name\") or name\n",
    "    else:\n",
    "        runtime_user_id = user_id\n",
    "    \n",
    "    if mongo_uri and runtime_user_id:\n",
    "        if job:\n",
    "            job.meta[\"status\"] = \"saving_to_history\"\n",
    "            job.save_meta()\n",
    "        log(\"üíæ Saving to database...\")\n",
    "        try:\n",
    "            from pymongo import MongoClient\n",
    "            client = MongoClient(mongo_uri)\n",
    "            db = client.get_default_database()\n",
    "            doc = {\n",
    "                \"user_id\": runtime_user_id,\n",
    "                \"name\": runtime_name,\n",
    "                \"created_at\": datetime.now(timezone.utc),\n",
    "                \"params\": job_params,\n",
    "                \"result\": result,\n",
    "            }\n",
    "            db.generations.insert_one(doc)\n",
    "            log(\"   ‚úÖ Saved to database!\")\n",
    "        except Exception as exc:\n",
    "            log(f\"   ‚ö†Ô∏è Could not save to database: {exc}\")\n",
    "    \n",
    "    # Finalize job meta\n",
    "    if job:\n",
    "        job.meta[\"status\"] = \"completed\"\n",
    "        job.meta[\"completed_at\"] = datetime.now(timezone.utc).isoformat()\n",
    "        job.meta[\"logs\"] = logs\n",
    "        job.save_meta()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training job function loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start Worker\n",
    "\n",
    "Run this cell to start processing jobs. It will:\n",
    "- Listen to the Redis queue\n",
    "- Pick up training jobs\n",
    "- Process them on GPU\n",
    "- Send results back through Redis\n",
    "\n",
    "**Keep this running!** Your frontend will see jobs progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rq import Worker, Queue\n",
    "\n",
    "print(f\"üöÄ Starting worker '{WORKER_NAME}'...\")\n",
    "print(f\"üìã Listening to queue: {QUEUE_NAME}\")\n",
    "print(f\"‚è±Ô∏è  Poll interval: {POLL_INTERVAL}s\")\n",
    "print(\"=\"*50)\n",
    "print(\"Worker is now running! Submit jobs from your UI.\")\n",
    "print(\"Press the ‚¨õ stop button to terminate.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create and start worker\n",
    "worker = Worker(\n",
    "    [queue],\n",
    "    connection=redis_conn,\n",
    "    name=WORKER_NAME,\n",
    ")\n",
    "\n",
    "# This blocks and processes jobs forever\n",
    "worker.work(with_scheduler=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Troubleshooting\n",
    "\n",
    "### Connection Issues\n",
    "```python\n",
    "# Test Redis connection manually\n",
    "redis_conn.ping()  # Should return True\n",
    "```\n",
    "\n",
    "### Queue Status\n",
    "```python\n",
    "# Check queue status\n",
    "print(f\"Queued: {len(queue)}\")\n",
    "print(f\"Job IDs: {queue.job_ids}\")\n",
    "```\n",
    "\n",
    "### Check Workers\n",
    "```python\n",
    "# See all connected workers\n",
    "from rq import Worker\n",
    "workers = Worker.all(connection=redis_conn)\n",
    "for w in workers:\n",
    "    print(f\"{w.name}: {w.state}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
